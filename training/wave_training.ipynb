{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667db1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# 1. PARAMETERS\n",
    "BASE_PATH       = \"/content/drive/MyDrive/drone_datasets\"\n",
    "CLIP_LENGTH     = 8\n",
    "CLIP_STRIDE     = 8\n",
    "BATCH_SIZE      = 8\n",
    "SHUFFLE_BUFFER  = 5000\n",
    "VAL_SIZE        = 3000\n",
    "EPOCHS          = 300\n",
    "STEPS_PER_EPOCH = 200   # adjust to roughly (total_train_clips // BATCH_SIZE)\n",
    "VAL_STEPS       = 50    # adjust to (VAL_SIZE // BATCH_SIZE)\n",
    "\n",
    "# 2. CLIP GENERATOR (RGB + preprocess)\n",
    "def clip_generator(directory_path, label, clip_length=CLIP_LENGTH, clip_stride=CLIP_STRIDE):\n",
    "    patterns = [os.path.join(directory_path, '**', ext) \n",
    "                for ext in ('*.avi','*.mp4','*.mov')]\n",
    "    for pattern in patterns:\n",
    "        for video_path in glob.glob(pattern, recursive=True):\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            buffer = deque(maxlen=clip_length)\n",
    "            frame_idx = 0\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                # convert to RGB & resize\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (224,224))\n",
    "                buffer.append(frame)\n",
    "                frame_idx += 1\n",
    "                if frame_idx >= clip_length and (frame_idx - clip_length) % clip_stride == 0:\n",
    "                    clip = np.array(buffer, dtype='float32')\n",
    "                    clip = preprocess_input(clip)\n",
    "                    yield clip, label\n",
    "            cap.release()\n",
    "\n",
    "# 3. DATASET CREATION\n",
    "dirs_info = [\n",
    "    (os.path.join(BASE_PATH, \"KTH\",       \"wave\"),      1),\n",
    "    (os.path.join(BASE_PATH, \"KTH\",       \"not_wave\"),  0),\n",
    "    (os.path.join(BASE_PATH, \"HMDB51\",    \"wave\"),      1),\n",
    "    (os.path.join(BASE_PATH, \"HMDB51\",    \"not_wave\"),  0),\n",
    "    (os.path.join(BASE_PATH, \"UAV-Gesture\",\"wave\"),     1),\n",
    "    (os.path.join(BASE_PATH, \"UAV-Gesture\",\"not_wave\"), 0),\n",
    "]\n",
    "\n",
    "datasets = []\n",
    "for path, label in dirs_info:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda p=path, l=label: clip_generator(p, l),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec((CLIP_LENGTH,224,224,3), tf.float32),\n",
    "            tf.TensorSpec((), tf.int32),\n",
    "        )\n",
    "    )\n",
    "    datasets.append(ds)\n",
    "\n",
    "# interleave, shuffle, split\n",
    "mixed = tf.data.Dataset.sample_from_datasets(datasets, seed=42)\n",
    "shuffled = mixed.shuffle(SHUFFLE_BUFFER, reshuffle_each_iteration=True)\n",
    "\n",
    "val_ds   = shuffled.take(VAL_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "train_ds = shuffled.skip(VAL_SIZE)\\\n",
    "                    .batch(BATCH_SIZE)\\\n",
    "                    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. MODEL DEFINITION\n",
    "cnn_base = MobileNetV2(\n",
    "    weights='imagenet', include_top=False, \n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "# unfreeze last 20 layers for fine-tuning\n",
    "for layer in cnn_base.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "for layer in cnn_base.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "inputs = Input((CLIP_LENGTH,224,224,3))\n",
    "x = TimeDistributed(cnn_base)(inputs)\n",
    "x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
    "x = LSTM(64, dropout=0.3)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 5. CALLBACKS & TENSORBOARD LOGGING\n",
    "log_dir = os.path.join(\"logs/fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_wave_model.h5', save_best_only=True, monitor='val_loss'),\n",
    "    tb_callback\n",
    "]\n",
    "\n",
    "# 6. TRAINING\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7. EVALUATION\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "results = model.evaluate(val_ds, steps=VAL_STEPS, verbose=1)\n",
    "print(\"Validation metrics:\")\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "# 8. SAVE MODEL\n",
    "model.save('wave_sequence_model_final.h5')\n",
    "print(\"Final model saved as wave_sequence_model_final.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
