{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667db1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3. Parameters & Paths\n",
    "base_path       = \"/content/drive/MyDrive/drone_datasets\"\n",
    "clip_length     = 8\n",
    "clip_stride     = 8\n",
    "batch_size      = 4\n",
    "shuffle_buffer  = 3500\n",
    "validation_size = 3000\n",
    "\n",
    "# 4. Clip generator (streaming)\n",
    "def clip_generator(directory_path, label, clip_length=8, clip_stride=8):\n",
    "    patterns = [os.path.join(directory_path, '**', ext) for ext in ['*.avi', '*.mp4', '*.mov']]\n",
    "    for pattern in patterns:\n",
    "        for video_path in glob.glob(pattern, recursive=True):\n",
    "            print(f\"Processing: {video_path}\")\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            buffer = deque(maxlen=clip_length)\n",
    "            frame_idx = 0\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                buffer.append(frame)\n",
    "                frame_idx += 1\n",
    "                if frame_idx >= clip_length and (frame_idx - clip_length) % clip_stride == 0:\n",
    "                    clip = np.array(buffer, dtype='uint8')\n",
    "                    clip = preprocess_input(clip.astype('float32'))\n",
    "                    yield clip, label\n",
    "            cap.release()\n",
    "\n",
    "# 5. Weighted clip generator (streams with sample weight)\n",
    "def weighted_clip_generator(directory_path, label, weight, clip_length=8, clip_stride=8):\n",
    "    for clip, lbl in clip_generator(directory_path, label, clip_length, clip_stride):\n",
    "        yield clip, lbl, weight\n",
    "\n",
    "# 6. Build weighted tf.data.Datasets for each source\n",
    "dirs_info = [\n",
    "    (os.path.join(base_path, \"KTH\",       \"wave\"),      1, 1.0),\n",
    "    (os.path.join(base_path, \"KTH\",       \"not_wave\"),  0, 1.0),\n",
    "    (os.path.join(base_path, \"HMDB51\",    \"wave\"),      1, 1.0),\n",
    "    (os.path.join(base_path, \"HMDB51\",    \"not_wave\"),  0, 1.0),\n",
    "    (os.path.join(base_path, \"UAV-Gesture\",\"wave\"),     1, 5.0),\n",
    "    (os.path.join(base_path, \"UAV-Gesture\",\"not_wave\"), 0, 5.0),\n",
    "]\n",
    "\n",
    "datasets = []\n",
    "for path, label, weight in dirs_info:\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda p=path, l=label, w=weight: weighted_clip_generator(p, l, w, clip_length, clip_stride),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec((clip_length, 224, 224, 3), tf.float32),\n",
    "            tf.TensorSpec((), tf.int32),\n",
    "            tf.TensorSpec((), tf.float32),\n",
    "        )\n",
    "    )\n",
    "    datasets.append(ds)\n",
    "\n",
    "# 7. Interleave and initial shuffle\n",
    "mixed_ds = tf.data.Dataset.sample_from_datasets(datasets, seed=42)\n",
    "dshuffled = mixed_ds.shuffle(shuffle_buffer)\n",
    "\n",
    "# 8. Split into validation and training\n",
    "val_ds   = dshuffled.take(validation_size)\n",
    "train_raw = dshuffled.skip(validation_size)\n",
    "\n",
    "# 9. Per-epoch shuffle + batch + prefetch\n",
    "train_ds = (\n",
    "    train_raw\n",
    "    .shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 10. Model definition\n",
    "cnn_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "cnn_base.trainable = False\n",
    "\n",
    "inputs = Input((clip_length,224,224,3))\n",
    "x = TimeDistributed(cnn_base)(inputs)\n",
    "x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
    "x = LSTM(64, dropout=0.3)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_seq = Model(inputs, outputs)\n",
    "model_seq.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ],\n",
    "    run_eagerly=True\n",
    ")\n",
    "\n",
    "model_seq.summary()\n",
    "\n",
    "# 11. Training with callbacks\n",
    "def print_callback(epoch, logs):\n",
    "    print(f\"Epoch {epoch+1}: loss={logs['loss']:.4f}, acc={logs['accuracy']:.4f}, \"\n",
    "          f\"val_loss={logs.get('val_loss',0):.4f}, val_acc={logs.get('val_accuracy',0):.4f}\")\n",
    "\n",
    "print_cb  = LambdaCallback(on_epoch_end=print_callback)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model_seq.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=1,\n",
    "    # steps_per_epoch=15,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop, print_cb]\n",
    ")\n",
    "\n",
    "# 12. Plotting results\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss'); plt.xlabel('Epoch'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy'); plt.xlabel('Epoch'); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 13. Final evaluation & save\n",
    "results = model_seq.evaluate(val_ds)\n",
    "for name, val in zip(model_seq.metrics_names, results):\n",
    "    print(f\"{name}: {val:.4f}\")\n",
    "\n",
    "model_seq.save('wave_sequence_model_one_epoch.h5')\n",
    "print(\"Model saved as wave_sequence_model_one_epoch.h5\")\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
