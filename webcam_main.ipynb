{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50270cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/braeden/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-5-2 Python-3.9.21 torch-2.6.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m_v6 summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 20:41:33.432 python[42809:27469893] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam initialized.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Webcam released, exiting.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWebcam released, exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, prob_text, (\u001b[38;5;241m10\u001b[39m, FRAME_HEIGHT \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m     97\u001b[0m                     cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.6\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     99\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWebcam\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "YOLO_MODEL_PATH = 'weights/yolov5m.pt'\n",
    "WAVE_MODEL_PATH = 'weights/wave_sequence_model_final.h5'\n",
    "CONF_THRESHOLD  = 0.5\n",
    "FRAME_WIDTH     = 640\n",
    "FRAME_HEIGHT    = 480\n",
    "CLIP_LENGTH     = 8\n",
    "ROI_SIZE        = 224\n",
    "\n",
    "def load_models():\n",
    "    detector = torch.hub.load(\n",
    "        'ultralytics/yolov5', 'custom',\n",
    "        path=YOLO_MODEL_PATH\n",
    "    )\n",
    "    detector.conf = CONF_THRESHOLD\n",
    "    wave_model = tf.keras.models.load_model(WAVE_MODEL_PATH)\n",
    "    return detector, wave_model\n",
    "\n",
    "def init_camera():\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_ANY)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Could not open webcam\")\n",
    "    return cap\n",
    "\n",
    "def detect_person_box(detector, frame):\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = detector(img, size=640)\n",
    "    dets = results.xyxy[0].cpu().numpy()\n",
    "    persons = [d for d in dets if int(d[5]) == 0 and d[4] >= CONF_THRESHOLD]\n",
    "    if not persons:\n",
    "        return None\n",
    "    x1, y1, x2, y2, _, _ = max(persons, key=lambda d: d[4])\n",
    "    return map(int, (x1, y1, x2, y2))\n",
    "\n",
    "def main():\n",
    "    person_detector, wave_model = load_models()\n",
    "    print(\"Models loaded.\")\n",
    "    cap = init_camera()\n",
    "    print(\"Webcam initialized.\")\n",
    "\n",
    "    roi_buffer = deque(maxlen=CLIP_LENGTH)\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "    latest_prob = 0.0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            box = detect_person_box(person_detector, frame)\n",
    "\n",
    "            # draw bounding box and label if person detected\n",
    "            if box:\n",
    "                x1, y1, x2, y2 = box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                label = \"Person\"\n",
    "\n",
    "                # crop and buffer\n",
    "                crop_bgr = frame[y1:y2, x1:x2]\n",
    "                if crop_bgr.size == 0:\n",
    "                    roi_buffer.clear()\n",
    "                else:\n",
    "                    crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "                    roi = cv2.resize(crop_rgb, (ROI_SIZE, ROI_SIZE))\n",
    "                    roi_buffer.append(roi)\n",
    "\n",
    "                    if len(roi_buffer) == CLIP_LENGTH:\n",
    "                        clip = np.stack(roi_buffer, axis=0).astype('float32')\n",
    "                        clip = preprocess_input(clip)\n",
    "                        latest_prob = float(wave_model.predict(clip[None, ...])[0, 0])\n",
    "                        roi_buffer.clear()\n",
    "\n",
    "                cv2.putText(frame, label, (x1, max(y1 - 10, 20)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            else:\n",
    "                roi_buffer.clear()\n",
    "\n",
    "            # overlay latest wave probability at bottom-left\n",
    "            prob_text = f\"Wave prob: {latest_prob:.2f}\"\n",
    "            cv2.putText(frame, prob_text, (10, FRAME_HEIGHT - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('Webcam', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Webcam released, exiting.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diffusion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
